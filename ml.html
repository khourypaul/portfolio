<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Paul Khoury - Portfolio</title>
<style>
    body {
        font-family: sans-serif;
        background-color: white;
        margin: 0;
        padding: 0;
        line-height: 1.6;
        display: flex;
        min-height: 100vh;
    }
    a {
        color: #0645ad;
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }

    /* Left navbar */
    nav {
        background-color: white;
        width: 15%;
        padding: 75px 40px 40px 50px; /* extra top padding for "Contents" */
        box-sizing: border-box;
        position: sticky;
        top: 0;
        height: 100vh;
    }
    nav h3 {
        font-size: 1em;
        margin-top: 0;
        margin-bottom: 10px;
        background-color: transparent;
        padding-bottom: 4px;
        border-bottom: 1px solid #ccc; /* divider line */
    }

    nav ol {
        padding-left: 20px;
    }

    /* Main content container */
    .main-container {
        display: flex;
        width: 89%;
    }
    .article {
        width: 85%; /* widened from 70% */
        padding: 20px;
        max-width: 980px;
        position: relative;
    }

    /* Header */
    header h1 {
        margin: 0;
        font-size: 2.2em;      /* slightly larger */
        font-weight: normal;   /* remove bold */
        border-bottom: 1px solid #ccc;
        padding-bottom: 4px;
    }

    header p {
        margin: 0;
        font-size: 0.9em;
        color: #555;
    }

    /* Infobox inside main content */
    .infobox {
        background-color: #f8f9fa;
        border: 1px solid #a2a9b1;
        padding: 10px;
        font-size: 0.9em;

        /* ~55% of the body width (right side of the page, not counting nav) */
        width: 55%;

        float: right;
        margin: 80px 0 10px 20px; /* lowered from top */
        box-sizing: border-box;
    }
    .infobox h3 {
        margin-top: 0;
        background-color: #eaecf0;
        padding: 5px;
        text-align: center;
        font-size: 1em;
    }

    /* Carousel */
    .carousel {
        position: relative;
        overflow: hidden;
        margin: 10px auto;
        width: 100%;
    }

    .carousel-track {
        display: flex;
        gap: 10px;
        align-items: center;
        overflow-x: auto;
        scroll-snap-type: x mandatory;
        -webkit-overflow-scrolling: touch;
        padding: 6px 2px 10px 2px;
    }

    /* Hide scrollbar (still scrollable) */
    .carousel-track::-webkit-scrollbar { height: 0; }
    .carousel-track { scrollbar-width: none; }

    .carousel-slide {
        flex: 0 0 auto;
        scroll-snap-align: start;
    }

    /* Fixed height, variable width */
    .carousel-slide img {
        height: 290px;
        width: auto;
        display: block;
        object-fit: cover;
        background-color: #ddd;
        border: 1px solid #c8ccd1;
        cursor: pointer; /* shows it’s clickable */
    }

    /* Prev/Next buttons */
    .carousel-btn {
        cursor: pointer;
        position: absolute;
        top: 50%;
        transform: translateY(-50%);
        user-select: none;
        background: rgba(255,255,255,0.85);
        border: 1px solid #c8ccd1;
        border-radius: 3px;
        padding: 6px 8px;
        font-size: 16px;
        color: #333;
    }
    .carousel-btn.prev { left: 8px; }
    .carousel-btn.next { right: 8px; }

    /* Right margin */
    .right-margin {
        width: 8%;
    }

    /* Footer */
    footer {
        margin-top: 20px;
        padding-top: 10px;
        border-top: 1px solid #ccc;
        font-size: 0.8em;
        color: #555;
        clear: both;
    }

    /* Blank slate section placeholder */
    .blank-slate {
        margin-top: 18px;
    }

    /* ===== Modal (lightbox) ===== */
    .img-modal {
        display: none;
        position: fixed;
        z-index: 2000;
        left: 0;
        top: 0;
        width: 100%;
        height: 100%;
        background: rgba(0,0,0,0.77);
        padding: 30px;
        box-sizing: border-box;
    }

    .img-modal.open {
        display: flex;
        align-items: center;
        justify-content: center;
    }

    .img-modal-content {
        position: relative;
        max-width: 70vw;
        max-height: 70vh;
		transform: translateY(-6%);
    }

    .img-modal-content img {
        max-width: 70vw;
        max-height: 70vh;
        width: auto;
        height: auto;
        display: block;
        background: #fff;
    }

    .img-modal-close {
        position: absolute;
        top: -14px;
        right: -14px;
        width: 34px;
        height: 34px;
        border-radius: 50%;
        border: 1px solid #c8ccd1;
        background: rgba(255,255,255,0.95);
        color: #333;
        font-size: 22px;
        line-height: 32px;
        text-align: center;
        cursor: pointer;
        user-select: none;
    }
	.img-modal-caption {
		margin-top: 10px;
		font-size: 0.9rem;
		color: #f0f0f0;
		text-align: center;
		max-width: 100ch;
	}

</style>
</head>
<body>

<!-- Left navbar -->
<nav>
    <h3>Contents</h3>
    <ol>
        <li><a href="index.html#intro">Home</a></li>
        <li><a href="index.html#about">About</a></li>
        <li><a href="index.html#experience">Experience</a></li>
        <li><a href="index.html#projects">Projects</a></li>
        <li><a href="index.html#awards">Awards</a></li>
        <li><a href="index.html#leadership">Leadership</a></li>
        <li><a href="index.html#skills">Skills</a></li>
        <li><a href="https://vsco.co/paulkhoury/gallery" target="_blank">Photography↗</a></li>
    </ol>
</nav>

<!-- Main content + right margin -->
<div class="main-container">
    <main class="article">
        <section id="intro">
            <!-- Infobox on the right inside main content -->
            <aside class="infobox">
                <h3>Image Gallery</h3>

                <div class="carousel" aria-label="Image carousel">
                    <div class="carousel-track" id="carouselTrack">
                        <!-- Replace these with your real images -->
                        <div class="carousel-slide"><img src="cm.png" alt="image 1" data-caption=""></div>
                        <div class="carousel-slide"><img src="tsne.png" alt="image 2" data-caption=""></div>
						<div class="carousel-slide"><img src="modelacc.png" alt="image 3" data-caption=""></div>
						
                    </div>

                    <button class="carousel-btn prev" id="carouselPrev" aria-label="Previous image">&#10094;</button>
                    <button class="carousel-btn next" id="carouselNext" aria-label="Next image">&#10095;</button>
				
                </div>
				<p>
					Click any image to view it at full resolution with additional context.
				</p>
            </aside>

            <header>
                <h1><span style="font-family: 'Georgia', serif;">Paul M. Khoury</span></h1>
                <p>A personal engineering portfolio webpage inspired by Wikipedia, the free encyclopedia.</p>
            </header>

            <div class="blank-slate">
                <h2>Predicting User Intent from EMG Signals</h2>
				<p>The primary objective of this project is to develop a machine learning (ML) model capable of accurately interpreting surface electromyographic (sEMG) signals to classify user intent, with applications in smart prosthetic control. sEMG signals capture muscle activation potentials non-invasively through electrodes placed on the skin. They offer a promising but complicated means of decoding voluntary limb movement. This technology forms the foundation for “smart” prosthetics, enabling artificial limbs that can truly act as natural biological limbs as far as actuation is concerned.</p>
<p>This research addresses key limitations in today's commercial prosthetic systems, which typically need manual setup, have limited movement capabilities, or don't work well for different users. Using supervised learning and improved signal analysis methods, this project develops an automated approach to recognize specific hand gestures from raw muscle signals. While earlier studies have shown good results with custom-designed features and simpler classification methods, this work compares traditional and newer machine learning approaches, tests how well they work across different situations, and carefully reviews assumptions made in previous research.</p>
<p>The ultimate goal, which is beyond the scope of this report, is contributing to the development of systems that can work in real-time and accommodate the full range of human motor abilities. If fully realized, this technology could seamlessly interpret “thoughts” controlling the performance of fine motor actions such as typing or sewing.</p>
<p>A comprehensive literature review informed key aspects of the study design. Kok et al. evaluated multiple feature extraction methods (MAV, RMS, DWT) and compared different classification models (SVM, KNN, Naïve Bayes) for anatomical motion prediction. Their work demonstrated the efficacy of statistical features like root mean square (RMS) and mean absolute value (MAV) for EMG classification, reinforcing the decision to apply time-domain statistical measures for collapsing temporal structures into formats suitable for time-independent classifiers.</p>
<p>Shakya and Ranjitkar explored convolutional neural networks (CNNs) and principal component analysis (PCA) for enhancing pattern recognition in forearm biomedical signals, providing insights into using multiple data forms to train classifiers. Their work with the GRABMyo dataset offered a useful reference point for data collection methodology.</p>
<p>Additionally, a review by Mhiriz et al. synthesized current challenges in sEMG-based classification, including inter-subject variability and noise robustness. Their paper addressed common issues like low signal-to-noise ratios and suggested methods to combat these problems using various models, including Support Vector Machines and Random Forests. This comprehensive overview guided preprocessing strategies and model selection for the project. </p>
<p>The dataset used originated from the UC Irvine Machine Learning Repository and was collected via the Myo Thalmic Bracelet, a commercially available (company is now defunct) sEMG acquisition device with eight EMG channels and consistent electrode placement. Ample data was collected and provided for analyses.</p>
<p>The project began with thorough data preparation and cleaning. Millions of raw data points were refined into a more manageable dataset by removing noise and calculating statistical features from each channel. The data was then standardized and visualized to better understand the patterns among different hand gestures.</p>
<p>Starting with a support vector machine model as a baseline, the project evaluated performance using standard accuracy metrics. This approach drew on techniques identified in the literature review. Later, additional features were tested and performance was finally compared with a neural network model.</p>
<p>Results showed the neural network achieved 94% accuracy, outperforming both the published benchmark and the baseline model. These findings confirm that even relatively simple statistical features from muscle signals can produce highly accurate gesture classification when paired with the right models. However, questions about how well these models work across different people remain open and will be explored in future work.</p>
<p>This project demonstrates the feasibility of creating intelligent prosthetic control systems using machine learning to interpret sEMG signals. The report details the process and the results.</p>
				<p><a href="proj_prog_5_final_results.ipynb" download>Download Jupyter Notebook</a></p>
            </div>
        </section>

        <footer>
            This site’s styling and layout were inspired by <a href="https://wikipedia.org">Wikipedia</a>. This portfolio is intended as a tongue-in-cheek homage and is not affiliated with Wikipedia or the Wikimedia Foundation.
        </footer>
    </main>

    <div class="right-margin"></div>
</div>

<!-- Modal -->
<div class="img-modal" id="imgModal" aria-hidden="true">
  <div class="img-modal-content" role="dialog" aria-modal="true" aria-label="Image preview">
    <div class="img-modal-close" id="imgModalClose" aria-label="Close">&times;</div>
    <img id="imgModalImage" src="" alt="">
	<p class="img-modal-caption" id="imgModalCaption"></p>
  </div>
</div>

<script>
  // Carousel scroll buttons
  const track = document.getElementById('carouselTrack');
  const prevBtn = document.getElementById('carouselPrev');
  const nextBtn = document.getElementById('carouselNext');

  function scrollByAmount(direction) {
    const firstImg = track.querySelector('.carousel-slide img');
    const imgWidth = firstImg ? firstImg.getBoundingClientRect().width : 300;
    const gap = 10;
    track.scrollBy({ left: direction * (imgWidth + gap), behavior: 'smooth' });
  }

  prevBtn.addEventListener('click', () => scrollByAmount(-1));
  nextBtn.addEventListener('click', () => scrollByAmount(1));

  // Modal logic
  const modal = document.getElementById('imgModal');
  const modalImg = document.getElementById('imgModalImage');
  const modalClose = document.getElementById('imgModalClose');

  // Open modal when any carousel image is clicked
	const modalCaption = document.getElementById('imgModalCaption');

	track.querySelectorAll('img').forEach(img => {
	  img.addEventListener('click', () => {
		modalImg.src = img.src;
		modalImg.alt = img.alt || '';
		modalCaption.textContent = img.dataset.caption || '';
		modal.classList.add('open');
		document.body.style.overflow = 'hidden';
	  });
	});

  function closeModal() {
    modal.classList.remove('open');
    modalImg.src = '';
    modalCaption.textContent = '';
    document.body.style.overflow = '';
  }


  // Close by X
  modalClose.addEventListener('click', closeModal);

  // Close by clicking outside the image (overlay)
  modal.addEventListener('click', (e) => {
    const clickedInside = e.target.closest('.img-modal-content');
    if (!clickedInside) closeModal();
  });

  // Close with Escape
  document.addEventListener('keydown', (e) => {
    if (e.key === 'Escape' && modal.classList.contains('open')) closeModal();
  });
</script>

</body>
</html>
